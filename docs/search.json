[
  {
    "objectID": "predictive_modeling.html",
    "href": "predictive_modeling.html",
    "title": "Homework 9: Modeling Practice",
    "section": "",
    "text": "Import libraries\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggplot2)\nlibrary(baguette)\nlibrary(rpart.plot)\n\n# suppress scientific notation\noptions(scipen=999)\nRead in data\ndf &lt;- read_csv('SeoulBikeData.csv', locale = locale(encoding = 'ISO-8859-1'))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View first few rows of data\nhead(df)\n\n# A tibble: 6 × 14\n  Date       `Rented Bike Count`  Hour `Temperature(°C)` `Humidity(%)`\n  &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n1 01/12/2017                 254     0              -5.2            37\n2 01/12/2017                 204     1              -5.5            38\n3 01/12/2017                 173     2              -6              39\n4 01/12/2017                 107     3              -6.2            40\n5 01/12/2017                  78     4              -6              36\n6 01/12/2017                 100     5              -6.4            37\n# ℹ 9 more variables: `Wind speed (m/s)` &lt;dbl&gt;, `Visibility (10m)` &lt;dbl&gt;,\n#   `Dew point temperature(°C)` &lt;dbl&gt;, `Solar Radiation (MJ/m2)` &lt;dbl&gt;,\n#   `Rainfall(mm)` &lt;dbl&gt;, `Snowfall (cm)` &lt;dbl&gt;, Seasons &lt;chr&gt;, Holiday &lt;chr&gt;,\n#   `Functioning Day` &lt;chr&gt;\nData Cleaning\ndf$Date &lt;- dmy(df$Date)\n\ndf &lt;- df |&gt;\n  # convert categorical variables to factors\n  mutate(across(c('Seasons', 'Holiday', 'Functioning Day'), as.factor)) |&gt;\n  # rename columns\n  rename('date' = 'Date',\n         'rented_bike_count' = 'Rented Bike Count',\n         'hour' = 'Hour',\n         'temp' = 'Temperature(°C)',\n         'humidity' = 'Humidity(%)',\n         'wind_speed' = 'Wind speed (m/s)',\n         'visibility' = 'Visibility (10m)',\n         'dew_pt_temp' = 'Dew point temperature(°C)',\n         'solar_rad' = 'Solar Radiation (MJ/m2)',\n         'rainfall' = 'Rainfall(mm)',\n         'snowfall' = 'Snowfall (cm)',\n         'season' = 'Seasons',\n         'holiday' = 'Holiday',\n         'functioning_day' = 'Functioning Day')\nSummarize variables across hours so that each day has one record\ndata &lt;- df |&gt;\n  group_by(date, season, holiday) |&gt;\n  summarize(\n    across(\n      c(rented_bike_count, rainfall, snowfall),\n      ~ sum(.x),\n      .names = 'total_{.col}'\n  ),\n    across(\n      c(temp, humidity, wind_speed, visibility, dew_pt_temp, solar_rad),\n      ~ mean(.x),\n      .names = 'mean_{.col}'),\n  .groups = 'drop')"
  },
  {
    "objectID": "predictive_modeling.html#predictive-modeling",
    "href": "predictive_modeling.html#predictive-modeling",
    "title": "Homework 9: Modeling Practice",
    "section": "Predictive Modeling",
    "text": "Predictive Modeling\nSet the seed to ensure results are consistent & reproducible\n\nset.seed(42)\n\n\nPrepare data for modeling\nSplit the data into a training and test set using 75% of the data for the training set\n\n# put 75% of the data in the training set\ndata_split &lt;- initial_split(data, prop = .75, strata = season)\n\n# create data frames for the two sets\ntrain &lt;- training(data_split)\ntest &lt;- testing(data_split)\n\nCreate 10 fold cross validation split on the training data\nThere will be 26 observations per fold, except for the last fold, which will have 29 observations.\n\n# set up 10 fold cross validation\ntrain_10_fold &lt;- vfold_cv(train, 10)\n\nCreate recipe to use in models\n\nrec &lt;- recipe(total_rented_bike_count ~ ., data = train) |&gt;\n  # create day of week variable\n  step_date(date, features = 'dow') |&gt;\n  # convert to weekday/weekend factor\n  step_mutate(\n    day_type = factor(\n      if_else(date_dow %in% c('Sat', 'Sun'), 'Weekend', 'Weekday')\n    )\n  ) |&gt;\n  # remove date and intermediate date variable created by step_date\n  step_rm(date, date_dow) |&gt;\n  # standardize numeric variables\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(season, holiday, day_type)\n\n\n\nLASSO Model\nCreate LASSO model instance and workflow\n\nUse ‘penalty = tune()’ to indicate that we are going to be tuning the model\nUse ‘mixture = 1’ to turn this into a LASSO model rather than elastic net\n\n\nLASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine('glmnet')\n\n# workflow\nLASSO_wkf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(LASSO_spec)\n\nFit tuned LASSO model & plot metrics across each fold\nWe know that each tuning value has the same RMSE, so use a small number of levels to reduce run time\n\nLASSO_grid &lt;- LASSO_wkf |&gt;\n  tune_grid(resamples = train_10_fold,\n            grid = grid_regular(penalty(), levels = 10),\n            metrics = metric_set(rmse, mae)) \n\n# plot metrics\nLASSO_grid |&gt;\n  collect_metrics() |&gt;\n  ggplot(aes(penalty, mean, color = .metric)) +\n  geom_line()\n\n\n\n\n\n\n\n\nSince all of the models perform the same, we will just select the first one to finalize\n\nLASSO_best &lt;- LASSO_grid |&gt;\n  collect_metrics() |&gt;\n  slice(1)\n\nFinalize the workflow fit the best model to the entire training set\n\nLASSO_final &lt;- LASSO_wkf |&gt;\n  finalize_workflow(LASSO_best) |&gt;\n  last_fit(data_split, metrics = metric_set(rmse, mae))\n\nLASSO_final |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard       6228. pre0_mod0_post0\n2 mae     standard       4127. pre0_mod0_post0\n\n\n\n\nRegression Tree Model\nCreate Regression Tree model instance and workflow\nThe first run of this model using tune() for all 3 parameters of decision_tree() took a long time to run and there was no improvement when using a tree depth higher than 8. Based on these results, I specified the value of tree depth to reduce run time.\n\nreg_tree &lt;- \n  decision_tree(\n    tree_depth = 8,\n    min_n = tune(),\n    cost_complexity = tune()) |&gt;\n  set_engine('rpart') |&gt;\n  set_mode('regression')\n\n# workflow\nreg_tree_wkf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(reg_tree)\n\nFit the workflow on CV folds\nUse 5 of each tuning parameter (cost_complexity and min_n)\n\nreg_tree_fit &lt;- reg_tree_wkf |&gt; \n  tune_grid(resamples = train_10_fold,\n            grid = grid_regular(min_n(),\n                                cost_complexity(),\n                                levels = c(5, 5)),\n            metrics = metric_set(rmse, mae))\n\n# view metrics for each fold\nreg_tree_fit |&gt;\n  collect_metrics(type = 'wide') |&gt;\n  arrange(rmse, mae)\n\n# A tibble: 25 × 5\n   cost_complexity min_n .config            mae  rmse\n             &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1    0.000562        40 pre0_mod20_post0 3716. 5406.\n 2    0.0000000001    40 pre0_mod05_post0 3716. 5406.\n 3    0.0000000178    40 pre0_mod10_post0 3716. 5406.\n 4    0.00000316      40 pre0_mod15_post0 3716. 5406.\n 5    0.0000000001    30 pre0_mod04_post0 3702. 5518.\n 6    0.0000000178    30 pre0_mod09_post0 3702. 5518.\n 7    0.00000316      30 pre0_mod14_post0 3702. 5518.\n 8    0.000562        30 pre0_mod19_post0 3699. 5518.\n 9    0.0000000001    21 pre0_mod03_post0 3785. 5595.\n10    0.0000000178    21 pre0_mod08_post0 3785. 5595.\n# ℹ 15 more rows\n\n\nPlot metrics\nThere is significant improvement in RMSE and MAE between min_n = 2 and min_n = 11. After that, we see much smaller differences between values of min_n with min_n = 40 having the lowest RMSE and MAE. We also see an uptick in RMSE and MAE between the values of 0.00001 and 0.01 for cost complexity (except for the RMSE from the model with min_n = 2, where RMSE falls between those two values).\n\nreg_tree_fit |&gt;\n  collect_metrics() |&gt;\n  mutate(min_n = factor(min_n)) |&gt;\n  ggplot(aes(cost_complexity, mean, color = min_n)) +\n  geom_line(size = 1.5, alpha = 0.6) +\n  geom_point(size = 2) +\n  facet_wrap(~ .metric, scales = 'free', nrow = 2) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = 'plasma', begin = .9, end = 0)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nSelect the model with the lowest RMSE to be finalized\n\nreg_tree_best &lt;- reg_tree_fit |&gt;\n  select_best(metric = 'rmse')\nreg_tree_best\n\n# A tibble: 1 × 3\n  cost_complexity min_n .config         \n            &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;           \n1        0.000562    40 pre0_mod20_post0\n\n\nFinalize the workflow with the best model and fit to the entire training set\n\nreg_tree_final &lt;- reg_tree_wkf |&gt;\n  finalize_workflow(reg_tree_best) |&gt;\n  last_fit(data_split, metrics = metric_set(rmse, mae))\n\nreg_tree_final |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard       6765. pre0_mod0_post0\n2 mae     standard       4801. pre0_mod0_post0\n\n\n\n\nBagged Tree Model\nCreate Bagged Tree model instance and workflow\nTuning all 3 parameters takes a very long time to run, so I ran it once to see what the best parameters for tree_depth and min_n were and then specified the values based off of those results.\n\nbag_spec &lt;- bag_tree(tree_depth = 10,\n                     min_n = 6,\n                     cost_complexity = tune()) |&gt;\n  set_engine('rpart') |&gt;\n  set_mode('regression')\n\n# workflow\nbag_wkf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(bag_spec)\n\nFit model to CV folds & view metrics across each fold\n\nbag_fit &lt;- bag_wkf |&gt;\n  tune_grid(resamples = train_10_fold,\n            grid = grid_regular(cost_complexity(), levels = 10),\n            metrics = metric_set(rmse, mae))\n\nRegistered S3 method overwritten by 'butcher':\n  method                 from    \n  as.character.dev_topic generics\n\nbag_fit |&gt;\n  collect_metrics(type = 'wide') |&gt;\n  arrange(rmse, mae)\n\n# A tibble: 10 × 4\n   cost_complexity .config            mae  rmse\n             &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1    0.000000001  pre0_mod02_post0 3193. 4885.\n 2    0.0000001    pre0_mod04_post0 3210. 4950.\n 3    0.00000001   pre0_mod03_post0 3243. 4984.\n 4    0.000001     pre0_mod05_post0 3195. 5005.\n 5    0.001        pre0_mod08_post0 3428. 5107.\n 6    0.0001       pre0_mod07_post0 3381. 5177.\n 7    0.0000000001 pre0_mod01_post0 3418. 5209.\n 8    0.00001      pre0_mod06_post0 3380. 5221.\n 9    0.01         pre0_mod09_post0 3694. 5457.\n10    0.1          pre0_mod10_post0 4615. 6209.\n\n\nSelect the model with the lowest RMSE to finalize\n\nbag_best &lt;- bag_fit |&gt;\n  select_best(metric = 'rmse')\nbag_best\n\n# A tibble: 1 × 2\n  cost_complexity .config         \n            &lt;dbl&gt; &lt;chr&gt;           \n1     0.000000001 pre0_mod02_post0\n\n\nFinalize the workflow using the best model and fit on the full training set\n\nbag_final &lt;- bag_wkf |&gt;\n  finalize_workflow(bag_best) |&gt;\n  last_fit(data_split, metrics = metric_set(rmse, mae))\n\nbag_final |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard       6005. pre0_mod0_post0\n2 mae     standard       3710. pre0_mod0_post0\n\n\n\n\nRandom Forest Model\nCreate Random Forest model instance and workflow\n\nUse importance = ‘impurity’ to request variable importance calculation\nSet all three parameter options to ‘tune()’\n\n\nrf_spec &lt;- \n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = tune()) |&gt;\n  set_engine('ranger', importance = 'impurity') |&gt;\n  set_mode('regression')\n\nrf_wkf &lt;- workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(rf_spec)\n\nFit model to CV folds & view metrics\n\nrf_fit &lt;- rf_wkf |&gt;\n  tune_grid(\n    resamples = train_10_fold,\n    grid = 10,\n    metrics = metric_set(rmse, mae))\n\ni Creating pre-processing data to finalize 1 unknown parameter: \"mtry\"\n\nrf_fit |&gt;\n  collect_metrics(type = 'wide') |&gt;\n  arrange(rmse, mae)\n\n# A tibble: 10 × 6\n    mtry trees min_n .config            mae  rmse\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1     5  1111     2 pre0_mod04_post0 3226. 4850.\n 2    11   889     6 pre0_mod09_post0 3166. 4876.\n 3     9  2000    14 pre0_mod07_post0 3257. 4923.\n 4    10   223    31 pre0_mod08_post0 3470. 5108.\n 5    13  1333    27 pre0_mod10_post0 3423. 5126.\n 6     7  1555    40 pre0_mod06_post0 3653. 5239.\n 7     3   445    35 pre0_mod03_post0 3894. 5384.\n 8     2  1777    23 pre0_mod02_post0 3953. 5440.\n 9     1   667    18 pre0_mod01_post0 4893. 6202.\n10     6     1    10 pre0_mod05_post0 4394. 6625.\n\n\nSelect the model with the lowest RMSE to finalize\n\nrf_best &lt;- rf_fit |&gt;\n  select_best(metric = 'rmse')\nrf_best\n\n# A tibble: 1 × 4\n   mtry trees min_n .config         \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;           \n1     5  1111     2 pre0_mod04_post0\n\n\nFinalize workflow for the best model and fit to the full training set\n\nrf_final &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best) |&gt;\n  last_fit(data_split, metrics = metric_set(rmse, mae))\n\nrf_final |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard       5613. pre0_mod0_post0\n2 mae     standard       3572. pre0_mod0_post0\n\n\n\n\nMLR Model\nCreate the recipe we used to create the best MLR model in homework 8\n\nmlr_rec &lt;- rec |&gt;\n  step_interact(~ starts_with('season_'):starts_with('holiday_') +\n                  starts_with('season_'):mean_temp +\n                  mean_temp:total_rainfall) |&gt; \n  step_poly(\n    all_numeric_predictors(),\n    # do not add quadratic term for dummy variables \n    -starts_with('season_'),\n    -starts_with('holiday_'),\n    -starts_with('day_type'),\n    degree = 2\n  )\n\nCreate model instance and workflow\n\nmlr_spec &lt;- linear_reg() |&gt;\n set_engine('lm')\n\nmlr_wkf &lt;- \n  workflow() |&gt;\n  add_model(mlr_spec) |&gt;\n  add_recipe(mlr_rec)\n\nFit model on CV folds and view metrics\n\n# fit model using 10 fold CV\nmlr_fit &lt;- mlr_wkf |&gt;\n  fit_resamples(\n    resamples = train_10_fold,\n    metrics = metric_set(rmse, mae)\n  )\n\n# view metrics\ncollect_metrics(mlr_fit, type = 'wide') |&gt; \n  arrange(rmse, mae)\n\n# A tibble: 1 × 3\n  .config           mae  rmse\n  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1 pre0_mod0_post0 3273. 5197.\n\n\nFit the model to the entire training set and view metrics\n\nMLR_final &lt;- mlr_wkf |&gt;\n  last_fit(data_split, metrics = metric_set(rmse, mae))\n\nMLR_final |&gt;\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config        \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 rmse    standard       5861. pre0_mod0_post0\n2 mae     standard       3330. pre0_mod0_post0"
  },
  {
    "objectID": "predictive_modeling.html#model-comparison",
    "href": "predictive_modeling.html#model-comparison",
    "title": "Homework 9: Modeling Practice",
    "section": "Model Comparison",
    "text": "Model Comparison\nCompare MAE and RMSE for each model\nThe random forest model has the lowest RMSE out of all of the models and the MLR model has the lowest MAE out of all the models.\nBased on these results, I will choose the random forest model as the best.\n\nrbind(\n  MLR_final |&gt;\n    collect_metrics(type = 'wide') |&gt; \n    mutate(Model = 'MLR', .before = 'rmse'),\n  LASSO_final |&gt;\n    collect_metrics(type = 'wide') |&gt; \n    mutate(Model = 'LASSO', .before = 'rmse'),\n  reg_tree_final |&gt;\n    collect_metrics(type = 'wide') |&gt; \n    mutate(Model = 'TREE', .before = 'rmse'),\n  bag_final |&gt; \n    collect_metrics(type = 'wide') |&gt; \n    mutate(Model = 'BAGGED', .before = 'rmse'),\n  rf_final |&gt;\n    collect_metrics(type = 'wide') |&gt; \n    mutate(Model = 'RF', .before = 'rmse')\n) |&gt;\n  arrange(rmse, mae)\n\n# A tibble: 5 × 4\n  .config         Model   rmse   mae\n  &lt;chr&gt;           &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 pre0_mod0_post0 RF     5613. 3572.\n2 pre0_mod0_post0 MLR    5861. 3330.\n3 pre0_mod0_post0 BAGGED 6005. 3710.\n4 pre0_mod0_post0 LASSO  6228. 4127.\n5 pre0_mod0_post0 TREE   6765. 4801."
  },
  {
    "objectID": "predictive_modeling.html#model-summaries",
    "href": "predictive_modeling.html#model-summaries",
    "title": "Homework 9: Modeling Practice",
    "section": "Model Summaries",
    "text": "Model Summaries\nLASSO Model coefficient table\n\nLASSO_final |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-10\n\n\n# A tibble: 14 × 3\n   term               estimate      penalty\n   &lt;chr&gt;                 &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)          17665. 0.0000000001\n 2 total_rainfall       -2011. 0.0000000001\n 3 total_snowfall        -380. 0.0000000001\n 4 mean_temp                0  0.0000000001\n 5 mean_humidity         -216. 0.0000000001\n 6 mean_wind_speed      -1236. 0.0000000001\n 7 mean_visibility       -204. 0.0000000001\n 8 mean_dew_pt_temp      2419. 0.0000000001\n 9 mean_solar_rad        4002. 0.0000000001\n10 season_Spring        -4055. 0.0000000001\n11 season_Summer         -449. 0.0000000001\n12 season_Winter        -7945. 0.0000000001\n13 holiday_No.Holiday    3265. 0.0000000001\n14 day_type_Weekend     -1865. 0.0000000001\n\n\nMLR Model coefficient table\n\nMLR_final |&gt;\n  extract_fit_parsnip() |&gt;\n  tidy()\n\n# A tibble: 30 × 5\n   term                               estimate std.error statistic      p.value\n   &lt;chr&gt;                                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)                          15610.     2672.    5.84   0.0000000164\n 2 season_Spring                          203.     3912.    0.0519 0.959       \n 3 season_Summer                        24232.     5397.    4.49   0.0000110   \n 4 season_Winter                         -898.     4823.   -0.186  0.853       \n 5 holiday_No.Holiday                    6036.     2356.    2.56   0.0110      \n 6 day_type_Weekend                     -1994.      698.   -2.86   0.00466     \n 7 season_Spring_x_holiday_No.Holiday   -3631.     3961.   -0.917  0.360       \n 8 season_Summer_x_holiday_No.Holiday   -2862.     4250.   -0.673  0.501       \n 9 season_Winter_x_holiday_No.Holiday   -4358.     3376.   -1.29   0.198       \n10 season_Spring_x_mean_temp             6884.     1969.    3.50   0.000560    \n# ℹ 20 more rows\n\n\nPlot of Regression Tree model final fit\n\nextract_fit_engine(reg_tree_final) |&gt;\n  rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n\n\nBagged Tree Model variable importance plot\nTemperature is the strongest predictor by far, which makes sense when looking at rented bike counts.\nDew point and solar radiation are higher in the ranking than I anticipated, but it makes sense since riding bikes is more common in dry, sunny weather.\nI am also surprised by the difference in importance between the seasons. Winter is much more valuable as a predictor than spring and summer are.\n\nbag_extract &lt;- extract_fit_engine(bag_final)\n\nbag_extract$imp |&gt;\n  mutate(term = factor(term, levels = term)) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat ='identity', fill = 'deepskyblue4') +\n  coord_flip()\n\n\n\n\n\n\n\n\nRandom Forest Model variable importance plot\nThis result is pretty similar to the bagged tree model variable importance plot. Temperature is the most important predictor and ‘holiday_No.Holiday’ is the least important, as was the case with the bagged tree model. In this model, season_Winter and dew point temperature have switched places and total snowfall has a lower importance.\n\nrf_extract &lt;- extract_fit_engine(rf_final)\n\nrf_imp &lt;- enframe(rf_extract$variable.importance,\n                  name = 'term',\n                  value = 'value')\n\nrf_imp |&gt;\n  mutate(term = reorder(term, value, decreasing = TRUE)) |&gt;\n  ggplot(aes(x = term, y = value)) +\n  geom_bar(stat ='identity', fill = 'deepskyblue4') +\n  coord_flip()"
  },
  {
    "objectID": "predictive_modeling.html#overall-best-fit",
    "href": "predictive_modeling.html#overall-best-fit",
    "title": "Homework 9: Modeling Practice",
    "section": "Overall best fit",
    "text": "Overall best fit\nThe random forest model performed the best on the test set, so now we will fit it to the entire data set\n\nrf &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best) |&gt;\n  fit(data)\n\nrf\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_normalize()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~5L,      x), num.trees = ~1111L, min.node.size = min_rows(~2L, x),      importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  1111 \nSample size:                      365 \nNumber of independent variables:  13 \nMtry:                             5 \nTarget node size:                 2 \nVariable importance mode:         impurity \nSplitrule:                        variance \nOOB prediction error (MSE):       25585155 \nR squared (OOB):                  0.7568868"
  }
]